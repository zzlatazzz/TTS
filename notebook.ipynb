{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "mvgssgg38yi1wq4v7sz5",
    "id": "scV7xMo2y9Zg"
   },
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "uc1th12fyui3oz215osnqo",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NF5O4gWS7UXW",
    "outputId": "1a1dbf68-03f9-43ab-e0e3-9ae4cdd96180"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "%%bash \n",
    "#install libraries\n",
    "pip install torchaudio\n",
    "pip install wandb\n",
    "pip install gdown\n",
    "pip install unidecode\n",
    "pip install inflect\n",
    "pip install --upgrade pydantic\n",
    "pip install seaborn\n",
    "pip install hparams\n",
    "\n",
    "#download LjSpeech\n",
    "wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2 -o /dev/null\n",
    "mkdir data\n",
    "tar -xvf LJSpeech-1.1.tar.bz2 >> /dev/null\n",
    "mv LJSpeech-1.1 data/LJSpeech-1.1\n",
    "\n",
    "gdown https://drive.google.com/u/0/uc?id=1-EdH0t0loc6vPiuVtXdhsDtzygWNSNZx\n",
    "mv train.txt data/\n",
    "\n",
    "#download Waveglow\n",
    "gdown https://drive.google.com/u/0/uc?id=1WsibBTsuRg_SF2Z6L6NFRTT-NjEy1oTx\n",
    "mkdir -p waveglow/pretrained_model/\n",
    "mv waveglow_256channels_ljs_v2.pt waveglow/pretrained_model/waveglow_256channels.pt\n",
    "\n",
    "# gdown https://drive.google.com/u/0/uc?id=1cJKJTmYd905a-9GFoo5gKjzhKjUVj83j\n",
    "# tar -xvf mel.tar.gz\n",
    "# echo $(ls mels | wc -l)\n",
    "\n",
    "#download alignments\n",
    "wget https://github.com/xcmyz/FastSpeech/raw/master/alignments.zip\n",
    "unzip alignments.zip >> /dev/null\n",
    "\n",
    "# we will use waveglow code, data and audio preprocessing from this repo\n",
    "git clone https://github.com/xcmyz/FastSpeech.git\n",
    "mv FastSpeech/text .\n",
    "mv FastSpeech/audio .\n",
    "mv FastSpeech/waveglow/* waveglow/\n",
    "mv FastSpeech/utils.py .\n",
    "mv FastSpeech/glow.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "0g4pc8z9sk7wapmfjjjyfwi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hq38NcDSDezg",
    "outputId": "0db58b4c-4f62-4af6-c503-c34137f88657"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "!head -n 5 data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "b2q286t5kwjn4iln5xgbwf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution_id": "eb42d942-1acb-44f9-b2c6-38cdd9195c82",
    "id": "RYUVJS9ynN1E",
    "outputId": "6dec7a27-4367-47b6-a623-8c9a024efaef"
   },
   "outputs": [],
   "source": [
    "# #!g1.4\n",
    "# import shutil\n",
    "\n",
    "# shutil.unpack_archive('energies.zip', 'energies')\n",
    "# shutil.unpack_archive('mels.zip', 'mels')\n",
    "# shutil.unpack_archive('pitches.zip', 'pitches')\n",
    "# shutil.unpack_archive('emotions.zip', 'emotions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "vvexm58g46hxp16ik2cp",
    "execution_id": "779668a4-300c-468f-b655-0d1c3eb79c06",
    "id": "3riM3InOy6Tj"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "xcx6uj18anr3kzp4dal9es",
    "id": "4xUE5OZaxtY_"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "import pathlib\n",
    "import random\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from IPython import display\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import distributions\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from collections import OrderedDict\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set()\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "import soundfile # read audio files\n",
    "import numpy as np\n",
    "import librosa # extract features\n",
    "import glob\n",
    "import os\n",
    "import pickle # to save model after training\n",
    "import time\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "hmwrrukxyag5wklrlfpum",
    "execution_id": "4ab517dc-1f2f-4a71-9f3e-eb0ce3e3bc2c",
    "id": "JJ5ItniqE78b"
   },
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "sjkikynlj7g68wn9t1kua7",
    "id": "aBSrJr9I8H0O"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "@dataclass\n",
    "class MelSpectrogramConfig:\n",
    "    num_mels = 80\n",
    "\n",
    "@dataclass\n",
    "class FastSpeechConfig:\n",
    "    vocab_size = 300\n",
    "    max_seq_len = 3000\n",
    "\n",
    "    encoder_dim = 256\n",
    "    encoder_n_layer = 4\n",
    "    encoder_head = 2\n",
    "    encoder_conv1d_filter_size = 1024\n",
    "\n",
    "    decoder_dim = 256\n",
    "    decoder_n_layer = 4\n",
    "    decoder_head = 2\n",
    "    decoder_conv1d_filter_size = 1024\n",
    "\n",
    "    fft_conv1d_kernel = (9, 1)\n",
    "    fft_conv1d_padding = (4, 0)\n",
    "\n",
    "    predictor_filter_size = 256\n",
    "    predictor_kernel_size = 3\n",
    "    predictor_dropout = 0.5\n",
    "    \n",
    "    dropout = 0.1\n",
    "    \n",
    "    PAD = 0\n",
    "    UNK = 1\n",
    "    BOS = 2\n",
    "    EOS = 3\n",
    "\n",
    "    PAD_WORD = '<blank>'\n",
    "    UNK_WORD = '<unk>'\n",
    "    BOS_WORD = '<s>'\n",
    "    EOS_WORD = '</s>'\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    checkpoint_path = \"./model_new\"\n",
    "    logger_path = \"./logger\"\n",
    "    mel_ground_truth = \"./mels/mels\"\n",
    "    energy_path =  \"./energies/energies\" #new\n",
    "    pitch_path =  \"./pitches/pitches\" #new\n",
    "    emotion_path =  \"./emotions/emotions\" #new\n",
    "    alignment_path = \"./alignments\"\n",
    "    data_path = './data/train.txt'\n",
    "    \n",
    "    wandb_project = 'fastspeech_example'\n",
    "    \n",
    "    text_cleaners = ['english_cleaners']\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = 'cuda:0'\n",
    "\n",
    "    batch_size = 16\n",
    "    epochs = 200\n",
    "    n_warm_up_step = 4000\n",
    "\n",
    "    learning_rate = 1e-3\n",
    "    weight_decay = 1e-6\n",
    "    grad_clip_thresh = 1.0\n",
    "    decay_step = [500000, 1000000, 2000000]\n",
    "\n",
    "    save_step = 3000\n",
    "    log_step = 5\n",
    "    clear_Time = 20\n",
    "\n",
    "    batch_expand_size = 32\n",
    "    \n",
    "\n",
    "mel_config = MelSpectrogramConfig()\n",
    "model_config = FastSpeechConfig()\n",
    "train_config = TrainConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "eefe5huunm4bhky4lc4xy",
    "id": "pHzieC44FAo8"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "from text import text_to_sequence\n",
    "\n",
    "\n",
    "def pad_1D(inputs, PAD=0):\n",
    "\n",
    "    def pad_data(x, length, PAD):\n",
    "        x_padded = np.pad(x, (0, length - x.shape[0]),\n",
    "                          mode='constant',\n",
    "                          constant_values=PAD)\n",
    "        return x_padded\n",
    "\n",
    "    max_len = max((len(x) for x in inputs))\n",
    "    padded = np.stack([pad_data(x, max_len, PAD) for x in inputs])\n",
    "\n",
    "    return padded\n",
    "\n",
    "\n",
    "def pad_1D_tensor(inputs, PAD=0):\n",
    "\n",
    "    def pad_data(x, length, PAD):\n",
    "        x_padded = F.pad(x, (0, length - x.shape[0]))\n",
    "        return x_padded\n",
    "\n",
    "    max_len = max((len(x) for x in inputs))\n",
    "    padded = torch.stack([pad_data(x, max_len, PAD) for x in inputs])\n",
    "\n",
    "    return padded\n",
    "\n",
    "\n",
    "def pad_2D(inputs, maxlen=None):\n",
    "\n",
    "    def pad(x, max_len):\n",
    "        PAD = 0\n",
    "        if np.shape(x)[0] > max_len:\n",
    "            raise ValueError(\"not max_len\")\n",
    "\n",
    "        s = np.shape(x)[1]\n",
    "        x_padded = np.pad(x, (0, max_len - np.shape(x)[0]),\n",
    "                          mode='constant',\n",
    "                          constant_values=PAD)\n",
    "        return x_padded[:, :s]\n",
    "\n",
    "    if maxlen:\n",
    "        output = np.stack([pad(x, maxlen) for x in inputs])\n",
    "    else:\n",
    "        max_len = max(np.shape(x)[0] for x in inputs)\n",
    "        output = np.stack([pad(x, max_len) for x in inputs])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def pad_2D_tensor(inputs, maxlen=None):\n",
    "\n",
    "    def pad(x, max_len):\n",
    "        if x.size(0) > max_len:\n",
    "            raise ValueError(\"not max_len\")\n",
    "\n",
    "        s = x.size(1)\n",
    "        x_padded = F.pad(x, (0, 0, 0, max_len-x.size(0)))\n",
    "        return x_padded[:, :s]\n",
    "\n",
    "    if maxlen:\n",
    "        output = torch.stack([pad(x, maxlen) for x in inputs])\n",
    "    else:\n",
    "        max_len = max(x.size(0) for x in inputs)\n",
    "        output = torch.stack([pad(x, max_len) for x in inputs])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def process_text(train_text_path):\n",
    "    with open(train_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        txt = []\n",
    "        for line in f.readlines():\n",
    "            txt.append(line)\n",
    "\n",
    "        return txt\n",
    "\n",
    "\n",
    "def get_data_to_buffer(train_config):\n",
    "    buffer = list()\n",
    "    text = process_text(train_config.data_path)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    for i in tqdm(range(len(text))):\n",
    "\n",
    "        mel_gt_name = os.path.join(\n",
    "            train_config.mel_ground_truth, \"ljspeech-mel-%05d.npy\" % (i+1))\n",
    "        mel_gt_target = np.load(mel_gt_name)\n",
    "        duration = np.load(os.path.join(\n",
    "            train_config.alignment_path, str(i)+\".npy\"))\n",
    "        \n",
    "        energy_gt_name = os.path.join(\n",
    "            train_config.energy_path, \"ljspeech-energy-%05d.npy\" % (i+1)) # new\n",
    "        energy_gt_target = np.load(energy_gt_name)\n",
    "        \n",
    "        pitch_gt_name = os.path.join(\n",
    "            train_config.pitch_path, \"ljspeech-pitch-%05d.npy\" % (i+1)) # new\n",
    "        pitch_gt_target = np.load(pitch_gt_name)\n",
    "\n",
    "        emotion_gt_name = os.path.join(\n",
    "            train_config.emotion_path, \"ljspeech-emotion-%05d.npy\" % (i+1)) # new\n",
    "        emotion_gt_target = np.load(emotion_gt_name)\n",
    "        \n",
    "        character = text[i][0:len(text[i])-1]\n",
    "        character = np.array(\n",
    "            text_to_sequence(character, train_config.text_cleaners))\n",
    "\n",
    "        character = torch.from_numpy(character)\n",
    "        duration = torch.from_numpy(duration)\n",
    "        mel_gt_target = torch.from_numpy(mel_gt_target)\n",
    "\n",
    "        energy_gt_target = torch.from_numpy(energy_gt_target) # new\n",
    "        pitch_gt_target = torch.from_numpy(pitch_gt_target) # new\n",
    "        emotion_gt_target = torch.from_numpy(emotion_gt_target) # new\n",
    "\n",
    "        buffer.append({\"text\": character, \"duration\": duration,\n",
    "                       \"mel_target\": mel_gt_target, \"energy\": energy_gt_target,\n",
    "                       \"pitch\": pitch_gt_target, \"emotion\": emotion_gt_target}) # new\n",
    "        \n",
    "    # normalize energy and pitch\n",
    "        \n",
    "    en = []\n",
    "    pit = []\n",
    "    em = []\n",
    "    for b in buffer:\n",
    "        en.append(b['energy'].mean())\n",
    "        \n",
    "        pit.append(b['pitch'].mean())\n",
    "        \n",
    "        #em.append(b['emotion'].mean())\n",
    "        \n",
    "    max_energy= np.mean(en)\n",
    "    max_pitch = np.mean(pit)\n",
    "    max_emotion = np.mean(em)\n",
    "    for i in range(len(buffer)):\n",
    "        buffer[i]['energy'] = buffer[i]['energy']/ max_energy\n",
    "        buffer[i]['pitch'] = buffer[i]['pitch'] / max_pitch\n",
    "        #buffer[i]['emotion'] = buffer[i]['emotion'] / max_emotion\n",
    "        \n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print(\"cost {:.2f}s to load all data into buffer.\".format(end-start))\n",
    "\n",
    "    return buffer\n",
    "\n",
    "\n",
    "class BufferDataset(Dataset):\n",
    "    def __init__(self, buffer):\n",
    "        self.buffer = buffer\n",
    "        self.length_dataset = len(self.buffer)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.buffer[idx]\n",
    "\n",
    "\n",
    "def reprocess_tensor(batch, cut_list):\n",
    "    texts = [batch[ind][\"text\"] for ind in cut_list]\n",
    "    mel_targets = [batch[ind][\"mel_target\"] for ind in cut_list]\n",
    "    durations = [batch[ind][\"duration\"] for ind in cut_list]\n",
    "\n",
    "    energies = [batch[ind][\"energy\"] for ind in cut_list] # new\n",
    "    pitches = [batch[ind][\"pitch\"] for ind in cut_list] # new\n",
    "    emotions = [batch[ind][\"emotion\"] for ind in cut_list] # new\n",
    "\n",
    "    length_text = np.array([])\n",
    "    for text in texts:\n",
    "        length_text = np.append(length_text, text.size(0))\n",
    "\n",
    "    src_pos = list()\n",
    "    max_len = int(max(length_text))\n",
    "    for length_src_row in length_text:\n",
    "        src_pos.append(np.pad([i+1 for i in range(int(length_src_row))],\n",
    "                              (0, max_len-int(length_src_row)), 'constant'))\n",
    "    src_pos = torch.from_numpy(np.array(src_pos))\n",
    "\n",
    "    length_mel = np.array(list())\n",
    "    for mel in mel_targets:\n",
    "        length_mel = np.append(length_mel, mel.size(0))\n",
    "\n",
    "    mel_pos = list()\n",
    "    max_mel_len = int(max(length_mel))\n",
    "    for length_mel_row in length_mel:\n",
    "        mel_pos.append(np.pad([i+1 for i in range(int(length_mel_row))],\n",
    "                              (0, max_mel_len-int(length_mel_row)), 'constant'))\n",
    "    mel_pos = torch.from_numpy(np.array(mel_pos))\n",
    "\n",
    "    texts = pad_1D_tensor(texts)\n",
    "    durations = pad_1D_tensor(durations)\n",
    "    mel_targets = pad_2D_tensor(mel_targets)\n",
    "\n",
    "    energies = pad_1D_tensor(energies) # new\n",
    "    pitches = pad_1D_tensor(pitches) # new\n",
    "    emotions = pad_1D_tensor(emotions) # new\n",
    "\n",
    "    out = {\"text\": texts,\n",
    "           \"mel_target\": mel_targets,\n",
    "           \"duration\": durations,\n",
    "           \"energy\": energies,\n",
    "           \"pitch\": pitches,\n",
    "           \"emotion\": emotions, \n",
    "           \"mel_pos\": mel_pos,\n",
    "           \"src_pos\": src_pos,\n",
    "           \"mel_max_len\": max_mel_len}\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def collate_fn_tensor(batch):\n",
    "    len_arr = np.array([d[\"text\"].size(0) for d in batch])\n",
    "    index_arr = np.argsort(-len_arr)\n",
    "    batchsize = len(batch)\n",
    "    real_batchsize = batchsize // train_config.batch_expand_size\n",
    "\n",
    "    cut_list = list()\n",
    "    for i in range(train_config.batch_expand_size):\n",
    "        cut_list.append(index_arr[i*real_batchsize:(i+1)*real_batchsize])\n",
    "\n",
    "    output = list()\n",
    "    for i in range(train_config.batch_expand_size):\n",
    "        output.append(reprocess_tensor(batch, cut_list[i]))\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "quuk8dy1cmf170as5461q",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lnNKVWf6FEAW",
    "outputId": "adda9f96-2f35-4f8a-cdad-a293cfa94e77"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "buffer = get_data_to_buffer(train_config)\n",
    "\n",
    "dataset = BufferDataset(buffer)\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=train_config.batch_expand_size * train_config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_tensor,\n",
    "    drop_last=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "mztnm15hnf3fovf6wz3zj",
    "execution_id": "a79f18c3-8b6c-4dfd-8d79-bfcfb3da3c85",
    "id": "36K-HdvnGeFn"
   },
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "02nd0nww96aom2knjy80e7",
    "execution_id": "b2edc3ff-9572-4f58-b980-88da27f2b677",
    "id": "bqq3_KtrGyaN"
   },
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ccp8xw3jvlfvq2zstnxum",
    "execution_id": "56e70699-a33c-4042-9708-7eee66127322",
    "id": "HpVGjGlrH6ff"
   },
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "0f6h0nnpwb8g0yh2he4vo3e",
    "id": "H0qHakwAH80g"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        attn = torch.bmm(q, k.transpose(-1, -2)) / self.temperature\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = torch.masked_fill(attn, mask, -math.inf)\n",
    "        \n",
    "        attn = self.dropout(self.softmax(attn))\n",
    "        output = torch.bmm(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_x, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_x = d_x\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_x)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_x)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_x)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(\n",
    "            temperature=d_x**0.5) \n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Linear(n_head * d_x, d_model)\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "         # normal distribution initialization better than kaiming(default in pytorch)\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0,\n",
    "                        std=np.sqrt(2.0 / (self.d_model + self.d_x)))\n",
    "        nn.init.normal_(self.w_ks.weight, mean=0,\n",
    "                        std=np.sqrt(2.0 / (self.d_model + self.d_x)))\n",
    "        nn.init.normal_(self.w_vs.weight, mean=0,\n",
    "                        std=np.sqrt(2.0 / (self.d_model + self.d_x))) \n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        d_x, n_head = self.d_x, self.n_head\n",
    "\n",
    "        sz_b, len_x, _ = x.size()\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        q = self.w_qs(x).view(sz_b, len_x, n_head, d_x)\n",
    "        k = self.w_ks(x).view(sz_b, len_x, n_head, d_x)\n",
    "        v = self.w_vs(x).view(sz_b, len_x, n_head, d_x)\n",
    "\n",
    "        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_x, d_x)  # (n*b) x lq x dk\n",
    "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_x, d_x)  # (n*b) x lk x dk\n",
    "        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_x, d_x)  # (n*b) x lv x dv\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(n_head, 1, 1)  # (n*b) x .. x ..\n",
    "        output, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        output = output.view(n_head, sz_b, len_x, d_x)\n",
    "        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_x, -1)  # b x lq x (n*dv)\n",
    "\n",
    "        output = self.dropout(self.fc(output))\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "4sb9nxme3jaj3jph31rmrs",
    "execution_id": "8555b6b7-6a04-432e-a7c1-691568e8b5b3",
    "id": "0076v2c3P3Xs"
   },
   "source": [
    "### Positionwise Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "sd59bmjlg7gz91kxsq503",
    "id": "M-pYP2WXP2l-"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Use Conv1D\n",
    "        # position-wise\n",
    "        self.w_1 = nn.Conv1d(\n",
    "            d_in, d_hid, kernel_size=model_config.fft_conv1d_kernel[0], padding=model_config.fft_conv1d_padding[0])\n",
    "        # position-wise\n",
    "        self.w_2 = nn.Conv1d(\n",
    "            d_hid, d_in, kernel_size=model_config.fft_conv1d_kernel[1], padding=model_config.fft_conv1d_padding[1])\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_in)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = self.layer_norm(x)\n",
    "        output = output.transpose(1, 2)\n",
    "        \n",
    "        output = self.w_2(F.relu(self.w_1(output)))\n",
    "        output = output.transpose(1, 2)\n",
    "        output = self.dropout(output)\n",
    "        output = x + output #self.layer_norm(output + residual)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ruc9xye3ntq7zgr52vele",
    "execution_id": "53d458a5-1ca9-4733-8671-bf9ec7b3abc6",
    "id": "WgGildSyP5_o"
   },
   "source": [
    "### FFTBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "55w9efl266hnde4a2myrv",
    "id": "0fYII3lQP_UD"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "class FFTBlock(torch.nn.Module):\n",
    "    \"\"\"FFT Block\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 d_inner,\n",
    "                 n_head,\n",
    "                 d_x,\n",
    "                 dropout=0.1):\n",
    "        super(FFTBlock, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(\n",
    "            n_head, d_model, d_x, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(\n",
    "            d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, non_pad_mask=None, slf_attn_mask=None):\n",
    "        enc_output, enc_slf_attn = self.slf_attn(\n",
    "            enc_input, mask=slf_attn_mask)\n",
    "        \n",
    "        if non_pad_mask is not None:\n",
    "            enc_output *= non_pad_mask\n",
    "\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        \n",
    "        if non_pad_mask is not None:\n",
    "            enc_output *= non_pad_mask\n",
    "        \n",
    "        return enc_output, enc_slf_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "494et6pfapuddsu22rxa1j",
    "execution_id": "59193275-dba1-4019-abcf-e3935a12a28a",
    "id": "Fq48hfKFdAbV"
   },
   "source": [
    "## Length Regulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "jxyp87rh64enqsidac4k1k",
    "execution_id": "c90a1a49-385c-4adf-a06e-0c7c7f905d8a",
    "id": "MleReDfIdENP"
   },
   "source": [
    "### Aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "b8oe7n5b895bwkklfk79nf",
    "id": "RKMo99RPdBIT"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "def create_alignment(base_mat, duration_predictor_output):\n",
    "    N, L = duration_predictor_output.shape\n",
    "    for i in range(N):\n",
    "        count = 0\n",
    "        for j in range(L):\n",
    "            for k in range(duration_predictor_output[i][j]):\n",
    "                base_mat[i][count+k][j] = 1\n",
    "            count = count + duration_predictor_output[i][j]\n",
    "    return base_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "v2mivsbz73hgj0wka7s09k",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7Ore-qic32k",
    "outputId": "94430244-da47-44e4-d51d-d102989d44c7"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "create_alignment(\n",
    "    torch.zeros(1, 6, 3).numpy(),\n",
    "    torch.LongTensor([[1,2,3]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ys6078ynuxnpu6rbnbvkm",
    "execution_id": "86a0de1b-8bae-43ea-82aa-7ead22232d63",
    "id": "S9bVS82Hdh6Z"
   },
   "source": [
    "### Duration Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "jogkxolrpyn7f9ol37vnq",
    "id": "V5Fcp9cNdijl"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self, dim_1, dim_2):\n",
    "        super().__init__()\n",
    "        self.dim_1 = dim_1\n",
    "        self.dim_2 = dim_2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.transpose(self.dim_1, self.dim_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "m34v0fk81frrjkmluthqsa",
    "id": "78xNKWondq0Y"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "class Predictor(nn.Module):\n",
    "    \"\"\" Predictor \"\"\"\n",
    "\n",
    "    def __init__(self, model_config: FastSpeechConfig):\n",
    "        super(Predictor, self).__init__()\n",
    "\n",
    "        self.input_size = model_config.encoder_dim\n",
    "        self.filter_size = model_config.predictor_filter_size\n",
    "        self.kernel = model_config.predictor_kernel_size\n",
    "        self.conv_output_size = model_config.predictor_filter_size\n",
    "        self.dropout = model_config.predictor_dropout\n",
    "\n",
    "        self.conv_net = nn.Sequential(\n",
    "            Transpose(-1, -2),\n",
    "            nn.Conv1d(\n",
    "                self.input_size, self.filter_size,\n",
    "                kernel_size=self.kernel, padding=1\n",
    "            ),\n",
    "            Transpose(-1, -2),\n",
    "            nn.LayerNorm(self.filter_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            Transpose(-1, -2),\n",
    "            nn.Conv1d(\n",
    "                self.filter_size, self.filter_size,\n",
    "                kernel_size=self.kernel, padding=1\n",
    "            ),\n",
    "            Transpose(-1, -2),\n",
    "            nn.LayerNorm(self.filter_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "        self.linear_layer = nn.Linear(self.conv_output_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, encoder_output):\n",
    "        encoder_output = self.conv_net(encoder_output)\n",
    "            \n",
    "        out = self.linear_layer(encoder_output)\n",
    "        out = self.relu(out)\n",
    "        out = out.squeeze()\n",
    "        if not self.training:\n",
    "            out = out.unsqueeze(0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "rerz4e8p89zbhck1dnclb",
    "execution_id": "da143b3f-9a16-4304-a35e-9731e7d85366",
    "id": "3D93VGkBURm_"
   },
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "dguw7l1ng2llgwff1vguwr",
    "id": "0CP38yO8dxoP"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "class LengthRegulator(nn.Module):\n",
    "    \"\"\" Length Regulator \"\"\"\n",
    "\n",
    "    def __init__(self, model_config):\n",
    "        super(LengthRegulator, self).__init__()\n",
    "        self.duration_predictor = Predictor(model_config)\n",
    "\n",
    "    def LR(self, x, duration_predictor_output, mel_max_length=None):\n",
    "        expand_max_len = torch.max(\n",
    "            torch.sum(duration_predictor_output, -1), -1)[0]\n",
    "        alignment = torch.zeros(duration_predictor_output.size(0),\n",
    "                                expand_max_len,\n",
    "                                duration_predictor_output.size(1)).numpy()\n",
    "        alignment = create_alignment(alignment,\n",
    "                                     duration_predictor_output.cpu().numpy())\n",
    "        alignment = torch.from_numpy(alignment).to(x.device)\n",
    "\n",
    "        output = alignment @ x\n",
    "        if mel_max_length:\n",
    "            output = F.pad(\n",
    "                output, (0, 0, 0, mel_max_length-output.size(1), 0, 0))\n",
    "        return output\n",
    "\n",
    "    def forward(self, x, alpha=1.0, target=None, mel_max_length=None):\n",
    "        log_dur_predictor_output = self.duration_predictor(x)\n",
    "\n",
    "        if target is not None:\n",
    "            output = self.LR(x, target, mel_max_length)\n",
    "            return output, torch.exp(log_dur_predictor_output)\n",
    "        else:\n",
    "            dur_predictor_output = torch.exp(log_dur_predictor_output)\n",
    "            dur_predictor_output = ((dur_predictor_output * alpha + 0.5)).int()\n",
    "\n",
    "            output = self.LR(x, dur_predictor_output)\n",
    "\n",
    "            mel_pos = torch.stack(\n",
    "                [torch.Tensor([i+1 for i in range(output.size(1))])]\n",
    "            ).long().to(train_config.device)\n",
    "\n",
    "            return output, mel_pos\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "nadvrp4lwxqykm7rh7d3c",
    "id": "l_sKIX91E5Ht"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "# map emotion to dimensional model space\n",
    "circumplex_model = {\n",
    "    \"neutral\":0.2,\n",
    "    \"happy\":0.8,\n",
    "    \"sad\":-0.8,\n",
    "    \"angry\":-0.1,\n",
    "    \"fearful\":-0.3, \n",
    "    \"disgust\":-0.3, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "196ro0rl3ygwjmigl2vo9l",
    "id": "NnNv5knLbNDc"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "import torch.nn as nn\n",
    "class VarianceAdaptor(nn.Module):\n",
    "    \"\"\" Variance Adaptor \"\"\"\n",
    "\n",
    "    def __init__(self, model_config):\n",
    "        super(VarianceAdaptor, self).__init__()\n",
    "        self.pitch_predictor = Predictor(model_config)\n",
    "        self.energy_predictor = Predictor(model_config)\n",
    "        self.emotion_predictor = Predictor(model_config)\n",
    "\n",
    "    def forward(self, x, alpha_pitch=1.0, alpha_energy=1.0, preferred_emotion=None, target_pitch=None, target_energy=None, target_emotion=None, mel_max_length=None):\n",
    "        ### Your code here\n",
    "        #dur_predictor_output = self.duration_predictor(x)\n",
    "        pitch_predictor_output = self.pitch_predictor(x)\n",
    "        energy_predictor_output = self.energy_predictor(x)\n",
    "        emotion_predictor_output = self.emotion_predictor(x)\n",
    "        #print(x.shape, pitch_predictor_output.shape, target_pitch.shape)\n",
    "        if target_pitch is not None:\n",
    "            p_vals = target_pitch.quantile(torch.tensor([i/256 for i in range(256)]).to(target_pitch.dtype).to(target_pitch.device)).reshape((1, 256))\n",
    "            target_pitch_ohe = torch.abs(target_pitch[:,:,None].expand((target_pitch.shape[0], target_pitch.shape[1], 256)) - p_vals[:,None,].expand((target_pitch.shape[0], target_pitch.shape[1], 256)))\n",
    "            target_pitch_ohe = target_pitch_ohe.view(-1, target_pitch_ohe.shape[-1])\n",
    "            ones = target_pitch_ohe.argmin(dim=-1)\n",
    "            target_pitch_ohe = torch.zeros_like(target_pitch_ohe)\n",
    "            target_pitch_ohe[torch.arange(target_pitch_ohe.shape[0]), ones] = 1\n",
    "            target_pitch_ohe = target_pitch_ohe.view(x.shape[0], -1, 256)\n",
    "            \n",
    "            \n",
    "            p_vals = target_energy.quantile(torch.tensor([i/256 for i in range(256)]).to(target_energy.dtype).to(target_energy.device)).reshape((1, 256))\n",
    "            target_energy_ohe = torch.abs(target_energy[:,:,None].expand((target_energy.shape[0], target_energy.shape[1], 256)) - p_vals[:,None,].expand((target_energy.shape[0], target_energy.shape[1], 256)))\n",
    "            target_energy_ohe = target_energy_ohe.view(-1, target_energy_ohe.shape[-1])\n",
    "            ones = target_energy_ohe.argmin(dim=-1)\n",
    "            target_energy_ohe = torch.zeros_like(target_energy_ohe)\n",
    "            target_energy_ohe[torch.arange(target_energy_ohe.shape[0]), ones] = 1\n",
    "            target_energy_ohe = target_energy_ohe.view(x.shape[0], -1, 256)\n",
    "            \n",
    "            print(x.shape, target_energy_ohe.shape, energy_predictor_output.shape, target_energy.shape)\n",
    "            \n",
    "            p_vals = target_emotion.quantile(torch.tensor([i/256 for i in range(256)]).to(target_emotion.dtype).to(target_emotion.device)).reshape((1, 256))\n",
    "            target_emotion_ohe = torch.abs(target_emotion[:,:,None].expand((target_emotion.shape[0], target_emotion.shape[1], 256)) - p_vals[:,None,].expand((target_emotion.shape[0], target_emotion.shape[1], 256)))\n",
    "            target_emotion_ohe = target_emotion_ohe.view(-1, target_emotion_ohe.shape[-1])\n",
    "            ones = target_emotion_ohe.argmin(dim=-1)\n",
    "            target_emotion_ohe = torch.zeros_like(target_emotion_ohe)\n",
    "            target_emotion_ohe[torch.arange(target_emotion_ohe.shape[0]), ones] = 1\n",
    "            target_emotion_ohe = target_emotion_ohe.view(x.shape[0], -1, 256)\n",
    "            \n",
    "            output = x + target_pitch_ohe + target_energy_ohe + target_emotion_ohe\n",
    "            \n",
    "            if mel_max_length:\n",
    "                output = F.pad(output, (0, 0, 0, mel_max_length-output.size(1), 0, 0))\n",
    "            return output, pitch_predictor_output, energy_predictor_output, emotion_predictor_output\n",
    "\n",
    "        else:\n",
    "            pitch_predictor_output = pitch_predictor_output * alpha_pitch\n",
    "            p_vals = pitch_predictor_output.quantile(torch.tensor([i/256 for i in range(256)]).to(pitch_predictor_output.dtype).to(pitch_predictor_output.device)).reshape((1, 256))\n",
    "            pitch_predictor_output_ohe = torch.abs(pitch_predictor_output[:,:,None].expand((pitch_predictor_output.shape[0], pitch_predictor_output.shape[1], 256)) - p_vals[:,None,].expand((pitch_predictor_output.shape[0], pitch_predictor_output.shape[1], 256)))\n",
    "            pitch_predictor_output_ohe = pitch_predictor_output_ohe.view(-1, pitch_predictor_output_ohe.shape[-1])\n",
    "            ones = pitch_predictor_output_ohe.argmin(dim=-1)\n",
    "            pitch_predictor_output_ohe = torch.zeros_like(pitch_predictor_output_ohe)\n",
    "            pitch_predictor_output_ohe[torch.arange(pitch_predictor_output_ohe.shape[0]), ones] = 1\n",
    "            pitch_predictor_output_ohe = pitch_predictor_output_ohe.view(x.shape[0], -1, 256)\n",
    "            \n",
    "            energy_predictor_output = energy_predictor_output * alpha_energy\n",
    "            p_vals = energy_predictor_output.quantile(torch.tensor([i/256 for i in range(256)]).to(energy_predictor_output.dtype).to(energy_predictor_output.device)).reshape((1, 256))\n",
    "            energy_predictor_output_ohe = torch.abs(energy_predictor_output[:,:,None].expand((energy_predictor_output.shape[0], energy_predictor_output.shape[1], 256)) - p_vals[:,None,].expand((energy_predictor_output.shape[0], energy_predictor_output.shape[1], 256)))\n",
    "            energy_predictor_output_ohe = energy_predictor_output_ohe.view(-1, energy_predictor_output_ohe.shape[-1])\n",
    "            ones = energy_predictor_output_ohe.argmin(dim=-1)\n",
    "            energy_predictor_output_ohe = torch.zeros_like(energy_predictor_output_ohe)\n",
    "            energy_predictor_output_ohe[torch.arange(energy_predictor_output_ohe.shape[0]), ones] = 1\n",
    "            energy_predictor_output_ohe = energy_predictor_output_ohe.view(x.shape[0], -1, 256)\n",
    "            \n",
    "            if preferred_emotion is not None:\n",
    "                emotion_predictor_output = 0.1 * emotion_predictor_output + 0.9 * circumplex_model[preferred_emotion]\n",
    "            p_vals = emotion_predictor_output.quantile(torch.tensor([i/256 for i in range(256)]).to(emotion_predictor_output.dtype).to(emotion_predictor_output.device)).reshape((1, 256))\n",
    "            emotion_predictor_output_ohe = torch.abs(emotion_predictor_output[:,:,None].expand((emotion_predictor_output.shape[0], emotion_predictor_output.shape[1], 256)) - p_vals[:,None,].expand((emotion_predictor_output.shape[0], emotion_predictor_output.shape[1], 256)))\n",
    "            emotion_predictor_output_ohe = emotion_predictor_output_ohe.view(-1, emotion_predictor_output_ohe.shape[-1])\n",
    "            ones = emotion_predictor_output_ohe.argmin(dim=-1)\n",
    "            emotion_predictor_output_ohe = torch.zeros_like(emotion_predictor_output_ohe)\n",
    "            emotion_predictor_output_ohe[torch.arange(emotion_predictor_output_ohe.shape[0]), ones] = 1\n",
    "            emotion_predictor_output_ohe = emotion_predictor_output_ohe.view(x.shape[0], -1, 256)\n",
    "            \n",
    "            \n",
    "            output = x + pitch_predictor_output_ohe + energy_predictor_output_ohe + emotion_predictor_output_ohe\n",
    "            \n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "py9kie3mqguyciz6frqkn",
    "execution_id": "32fbfe64-1679-4c39-b7c9-7102274c71a3",
    "id": "zG4U0paqd1b0"
   },
   "source": [
    "## Final BLock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "d0uak98sek6tz7tl7yrebh",
    "execution_id": "351f7b23-26f0-4e6c-9c15-aee62b008a43",
    "id": "x-XYvTrNd4mG"
   },
   "source": [
    "### Attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "bn5rgwp6o5qktsuou0oz2k",
    "id": "7JF2Th99d2DN"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "def get_non_pad_mask(seq):\n",
    "    assert seq.dim() == 2\n",
    "    return seq.ne(model_config.PAD).type(torch.float).unsqueeze(-1)\n",
    "\n",
    "def get_attn_key_pad_mask(seq_k, seq_q):\n",
    "    ''' For masking out the padding part of key sequence. '''\n",
    "    # Expand to fit the shape of key query attention matrix.\n",
    "    len_q = seq_q.size(1)\n",
    "    padding_mask = seq_k.eq(model_config.PAD)\n",
    "    padding_mask = padding_mask.unsqueeze(\n",
    "        1).expand(-1, len_q, -1)  # b x lq x lk\n",
    "\n",
    "    return padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "egbhd55a02vrvftli637j",
    "execution_id": "fcc36d4b-a114-44c2-8323-accc18b148a6",
    "id": "kgG-45_qeAXz"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "cmoft7w93qf7ifzndbx8su",
    "id": "bZO7bAeXeCir"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_config):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        len_max_seq=model_config.max_seq_len\n",
    "        n_position = len_max_seq + 1\n",
    "        n_layers = model_config.encoder_n_layer\n",
    "\n",
    "        self.src_word_emb = nn.Embedding(\n",
    "            model_config.vocab_size,\n",
    "            model_config.encoder_dim,\n",
    "            padding_idx=model_config.PAD\n",
    "        )\n",
    "\n",
    "        self.position_enc = nn.Embedding(\n",
    "            n_position,\n",
    "            model_config.encoder_dim,\n",
    "            padding_idx=model_config.PAD\n",
    "        )\n",
    "\n",
    "        self.layer_stack = nn.ModuleList([FFTBlock(\n",
    "            model_config.encoder_dim,\n",
    "            model_config.encoder_conv1d_filter_size,\n",
    "            model_config.encoder_head,\n",
    "            model_config.encoder_dim // model_config.encoder_head,\n",
    "            dropout=model_config.dropout\n",
    "        ) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, src_seq, src_pos, return_attns=False):\n",
    "\n",
    "        enc_slf_attn_list = []\n",
    "\n",
    "        # -- Prepare masks\n",
    "        slf_attn_mask = get_attn_key_pad_mask(seq_k=src_seq, seq_q=src_seq)\n",
    "        non_pad_mask = get_non_pad_mask(src_seq)\n",
    "        \n",
    "        # -- Forward\n",
    "        enc_output = self.src_word_emb(src_seq) + self.position_enc(src_pos)\n",
    "\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(\n",
    "                enc_output,\n",
    "                non_pad_mask=non_pad_mask,\n",
    "                slf_attn_mask=slf_attn_mask)\n",
    "            if return_attns:\n",
    "                enc_slf_attn_list += [enc_slf_attn]\n",
    "\n",
    "        return enc_output, non_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "4zmu1ecg9sx7bcbd06ion8",
    "id": "vtf8yEntd2F7"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\" Decoder \"\"\"\n",
    "\n",
    "    def __init__(self, model_config):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        len_max_seq=model_config.max_seq_len\n",
    "        n_position = len_max_seq + 1\n",
    "        n_layers = model_config.decoder_n_layer\n",
    "\n",
    "        self.position_enc = nn.Embedding(\n",
    "            n_position,\n",
    "            model_config.encoder_dim,\n",
    "            padding_idx=model_config.PAD,\n",
    "        )\n",
    "\n",
    "        self.layer_stack = nn.ModuleList([FFTBlock(\n",
    "            model_config.encoder_dim,\n",
    "            model_config.encoder_conv1d_filter_size,\n",
    "            model_config.encoder_head,\n",
    "            model_config.encoder_dim // model_config.encoder_head,\n",
    "            dropout=model_config.dropout\n",
    "        ) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_seq, enc_pos, return_attns=False):\n",
    "\n",
    "        dec_slf_attn_list = []\n",
    "\n",
    "        # -- Prepare masks\n",
    "        slf_attn_mask = get_attn_key_pad_mask(seq_k=enc_pos, seq_q=enc_pos)\n",
    "        non_pad_mask = get_non_pad_mask(enc_pos)\n",
    "\n",
    "        # -- Forward\n",
    "        dec_output = enc_seq + self.position_enc(enc_pos)\n",
    "\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output, dec_slf_attn = dec_layer(\n",
    "                dec_output,\n",
    "                non_pad_mask=non_pad_mask,\n",
    "                slf_attn_mask=slf_attn_mask)\n",
    "            if return_attns:\n",
    "                dec_slf_attn_list += [dec_slf_attn]\n",
    "\n",
    "        return dec_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "frvs65vwkok511yd517bj",
    "id": "pyn1pgOWeIUx"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "def get_mask_from_lengths(lengths, max_len=None):\n",
    "    if max_len == None:\n",
    "        max_len = torch.max(lengths).item()\n",
    "\n",
    "    ids = torch.arange(0, max_len, 1, device=lengths.device)\n",
    "    mask = (ids < lengths.unsqueeze(1)).bool()\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "bctgbgr1mnmydpn8eei3of",
    "id": "EtQnvg10eK-d"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "class FastSpeech2(nn.Module):\n",
    "    \"\"\" FastSpeech2 \"\"\"\n",
    "\n",
    "    def __init__(self, model_config):\n",
    "        super(FastSpeech2, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(model_config)\n",
    "        self.length_regulator = LengthRegulator(model_config)\n",
    "        self.variance_adaptor = VarianceAdaptor(model_config)\n",
    "        self.decoder = Decoder(model_config)\n",
    "\n",
    "        self.mel_linear = nn.Linear(model_config.decoder_dim, mel_config.num_mels)\n",
    "\n",
    "    def mask_tensor(self, mel_output, position, mel_max_length):\n",
    "        lengths = torch.max(position, -1)[0]\n",
    "        mask = ~get_mask_from_lengths(lengths, max_len=mel_max_length)\n",
    "        mask = mask.unsqueeze(-1).expand(-1, -1, mel_output.size(-1))\n",
    "        return mel_output.masked_fill(mask, 0.)\n",
    "\n",
    "    def forward(self, src_seq, src_pos, mel_pos=None, mel_max_length=None, length_target=None, target_pitch=None, target_energy=None, target_emotion=None, alpha=1.0, alpha_pitch=1.0, alpha_energy=1.0, preferred_emotion=None):\n",
    "        ### Your code here\n",
    "        x, non_pad_mask = self.encoder(src_seq, src_pos)\n",
    "\n",
    "        if self.training:\n",
    "            output, dur_predictor_output = self.length_regulator(x, alpha, length_target, mel_max_length)\n",
    "            output, pitch_predictor_output, energy_predictor_output, emotion_predictor_output = self.variance_adaptor(output, alpha_pitch, alpha_energy, preferred_emotion, target_pitch, target_energy, target_emotion, mel_max_length)\n",
    "            output = self.decoder(output, mel_pos)\n",
    "            output = self.mask_tensor(output, mel_pos, mel_max_length)\n",
    "            output = self.mel_linear(output)\n",
    "            return output, dur_predictor_output, pitch_predictor_output, energy_predictor_output, emotion_predictor_output\n",
    "\n",
    "        else:\n",
    "            output, mel_pos = self.length_regulator(x, alpha)\n",
    "            output = self.variance_adaptor(output, alpha_pitch, alpha_energy, preferred_emotion)\n",
    "            output = self.decoder(output, mel_pos)\n",
    "            output = self.mel_linear(output)\n",
    "\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "br3819hj95iv5bqyxkg8j",
    "execution_id": "3b70fa21-8ab5-4bd7-a52d-f205eb848f08",
    "id": "yu8V4Xo6eSRj"
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "0i4qewc9jt4usctfryb0p4j",
    "id": "_vM_J16geUb9"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FastSpeechLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(self, mel, duration_predicted, pitch_predicted, energy_predicted, emotion_predicted, mel_target, duration_predictor_target, pitch_predictor_target, energy_predictor_target, emotion_predictor_target):\n",
    "        mel_loss = self.mse_loss(mel, mel_target)\n",
    "\n",
    "        duration_predictor_loss = self.l1_loss(duration_predicted,\n",
    "                                               duration_predictor_target.float())\n",
    "        \n",
    "        pitch_predictor_loss = self.mse_loss(pitch_predicted,\n",
    "                                               pitch_predictor_target)\n",
    "        \n",
    "        energy_predictor_loss = self.mse_loss(energy_predicted,\n",
    "                                               energy_predictor_target)\n",
    "        \n",
    "        emotion_predictor_loss = self.mse_loss(emotion_predicted,\n",
    "                                               emotion_predictor_target)\n",
    "\n",
    "        return mel_loss, duration_predictor_loss, pitch_predictor_loss, energy_predictor_loss, emotion_predictor_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "8w379vsw6mk25zo731ljy8i",
    "execution_id": "e35c9bf6-7c6b-4af4-8d81-817045eb9ff2",
    "id": "MhVFFc7-eVLT"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "o8fks4w8j3ecoqern5ah",
    "id": "754Rq62CeXl3"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "from torch.optim.lr_scheduler  import OneCycleLR\n",
    "from wandb_writer import WanDBWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "cfjkwguf6n6wuwp9c34pk",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "oXJNU3wrecVu",
    "outputId": "b64c1530-bd4a-4c39-b6b4-10af47878baa"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "model = FastSpeech2(model_config)\n",
    "model.load_state_dict(torch.load('./model_new2/checkpoint_63000.pth.tar', map_location='cuda:0')['model'])\n",
    "model = model.to(train_config.device)\n",
    "\n",
    "fastspeech_loss = FastSpeechLoss()\n",
    "current_step = 0\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=train_config.learning_rate,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9)\n",
    "\n",
    "scheduler = OneCycleLR(optimizer, **{\n",
    "    \"steps_per_epoch\": len(training_loader) * train_config.batch_expand_size,\n",
    "    \"epochs\": train_config.epochs,\n",
    "    \"anneal_strategy\": \"cos\",\n",
    "    \"max_lr\": train_config.learning_rate,\n",
    "    \"pct_start\": 0.1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "2gmpa1r84256pxrez6xxiv",
    "id": "eKD-1SHheebf"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "logger = WanDBWriter(train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "aiqlay6mcogtuocf8mq00l",
    "execution_id": "5a7e671a-f785-46f7-9d97-e81cfa102b77",
    "id": "FkF_-lKOehDO"
   },
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "jve7q81cwfml57vnnoumls",
    "id": "k0tXCrQ_ejx1"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "tqdm_bar = tqdm(total=train_config.epochs * len(training_loader) * train_config.batch_expand_size - current_step)\n",
    "\n",
    "\n",
    "for epoch in range(train_config.epochs):\n",
    "    for i, batchs in enumerate(training_loader):\n",
    "        # real batch start here\n",
    "        for j, db in enumerate(batchs):\n",
    "            current_step += 1\n",
    "            tqdm_bar.update(1)\n",
    "            \n",
    "            logger.set_step(current_step)\n",
    "\n",
    "            # Get Data\n",
    "            character = db[\"text\"].long().to(train_config.device)\n",
    "            mel_target = db[\"mel_target\"].float().to(train_config.device)\n",
    "            duration = db[\"duration\"].int().to(train_config.device)\n",
    "            \n",
    "            energy = db[\"energy\"].float().to(train_config.device)\n",
    "            pitch = db[\"pitch\"].float().to(train_config.device)\n",
    "            emotion = db[\"emotion\"].float().to(train_config.device)\n",
    "            \n",
    "            mel_pos = db[\"mel_pos\"].long().to(train_config.device)\n",
    "            src_pos = db[\"src_pos\"].long().to(train_config.device)\n",
    "            max_mel_len = db[\"mel_max_len\"]\n",
    "            \n",
    "\n",
    "            # Forward\n",
    "            mel_output, duration_predictor_output, pitch_predictor_output, energy_predictor_output, emotion_predictor_output = model(character,\n",
    "                                                          src_pos,\n",
    "                                                          mel_pos=mel_pos,\n",
    "                                                          mel_max_length=max_mel_len,\n",
    "                                                          length_target=duration,\n",
    "                                                          target_pitch=pitch, target_energy=energy, target_emotion=emotion,\n",
    "                                                          )\n",
    "            \n",
    "            # Calc Loss\n",
    "            mel_loss, duration_loss, pitch_loss, energy_loss, emotion_loss = fastspeech_loss(mel_output,\n",
    "                                                    duration_predictor_output, \n",
    "                                                    pitch_predictor_output, # 868\n",
    "                                                    energy_predictor_output,\n",
    "                                                    emotion_predictor_output,\n",
    "                                                    mel_target,\n",
    "                                                    duration,\n",
    "                                                    pitch, # 868\n",
    "                                                    energy,\n",
    "                                                    emotion)\n",
    "            total_loss = mel_loss + duration_loss + pitch_loss + energy_loss + emotion_loss\n",
    "\n",
    "            # Logger\n",
    "            t_l = total_loss.detach().cpu().numpy()\n",
    "            m_l = mel_loss.detach().cpu().numpy()\n",
    "            d_l = duration_loss.detach().cpu().numpy()\n",
    "            p_l = pitch_loss.detach().cpu().numpy()\n",
    "            en_l = energy_loss.detach().cpu().numpy()\n",
    "            em_l = emotion_loss.detach().cpu().numpy()\n",
    "            \n",
    "            logger.add_scalar(\"emotion_loss\", em_l)\n",
    "            logger.add_scalar(\"energy\", en_l)\n",
    "            logger.add_scalar(\"pitch_loss\", p_l)\n",
    "            logger.add_scalar(\"duration_loss\", d_l)\n",
    "            logger.add_scalar(\"mel_loss\", m_l)\n",
    "            logger.add_scalar(\"total_loss\", t_l)\n",
    "\n",
    "            # Backward\n",
    "            total_loss.backward()\n",
    "\n",
    "            # Clipping gradients to avoid gradient explosion\n",
    "            nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), train_config.grad_clip_thresh)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            \n",
    "            clear_output()\n",
    "            \n",
    "            if current_step % train_config.save_step == 0:\n",
    "                torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict(\n",
    "                )}, os.path.join(train_config.checkpoint_path, 'checkpoint_%d.pth.tar' % current_step))\n",
    "                print(\"save model at step %d ...\" % current_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "v7o8uzqty3jpshj6ylqh3",
    "id": "KpYsd2ANel4c"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "import waveglow\n",
    "import text\n",
    "import audio\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "054ul848ahjonmkycdeh4xq",
    "id": "d0O1js7qeqCJ"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "WaveGlow = utils.get_WaveGlow()\n",
    "WaveGlow = WaveGlow.cuda()\n",
    "model.load_state_dict(torch.load('./model_new/checkpoint_9000.pth.tar', map_location='cuda:0')['model'])\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "cj65vl0uk22n4am72wy92",
    "id": "YjyBuMcher8U"
   },
   "outputs": [],
   "source": [
    "#!g1.4\n",
    "def synthesis(model, text, alpha=1.0, alpha_pitch=1.0, alpha_energy=1.0, emotion='neutral'):\n",
    "    text = np.array(phn)\n",
    "    text = np.stack([text])\n",
    "    src_pos = np.array([i+1 for i in range(text.shape[1])])\n",
    "    src_pos = np.stack([src_pos])\n",
    "    sequence = torch.from_numpy(text).long().to(train_config.device)\n",
    "    src_pos = torch.from_numpy(src_pos).long().to(train_config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mel = model.forward(sequence, src_pos, alpha=alpha, alpha_pitch=alpha_pitch, alpha_energy=alpha_energy, preferred_emotion=emotion)\n",
    "    return mel[0].cpu().transpose(0, 1), mel.contiguous().transpose(1, 2)\n",
    "\n",
    "def get_data():\n",
    "    tests = [ \n",
    "        \"A defibrillator is a device that gives a high energy electric shock to the heart of someone who is in cardiac arrest\",\n",
    "        \"Massachusetts Institute of Technology may be best known for its math, science and engineering education\",\n",
    "        \"Wasserstein distance or Kantorovich Rubinstein metric is a distance function defined between probability distributions on a given metric space\"\n",
    "        \n",
    "    ]\n",
    "    data_list = list(text.text_to_sequence(test, train_config.text_cleaners) for test in tests)\n",
    "\n",
    "    return data_list\n",
    "\n",
    "data_list = get_data()\n",
    "for alpha in [0.8, 1., 1.2]:\n",
    "    for alpha_pitch in [0.8, 1., 1.2]:\n",
    "        for alpha_energy in [0.8, 1., 1.2]:\n",
    "            for emotion in ['happy', 'sad', 'disgust']:\n",
    "                for i, phn in tqdm(enumerate(data_list)):\n",
    "                    mel, mel_cuda = synthesis(model, phn, alpha)\n",
    "        \n",
    "                    os.makedirs(\"results\", exist_ok=True)\n",
    "        \n",
    "                    waveglow.inference.inference(\n",
    "                        mel_cuda, WaveGlow,\n",
    "                        f\"results/s=speed{alpha}_pitch{alpha_pitch}_energy{alpha_energy}_{emotion}_{i}_waveglow.wav\"\n",
    "                    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9OOCUan0H4JQ",
    "HpVGjGlrH6ff",
    "izsTO1fRdSp6",
    "WgGildSyP5_o",
    "MleReDfIdENP"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "8b5a6475-293f-41ea-b260-e1249b189bfc",
  "notebookPath": "Untitled0.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
